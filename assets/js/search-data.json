{
  
    
        "post0": {
            "title": "2. Neural Network From Scratch",
            "content": "Neural Network From Scratch Using Pytorch and FAST.AI Capabilities . This tutorial is created from Lecture 4 from FAST ai Course Deep Learning from coders Course I will go through step by step how to build Classifier using pytorch from scratch. . Import Packages . Un comment &quot;!pip install -Uqq fastbook&quot; install if running colab . Download Dataset . Using Fastai function with untardata we are downloading images of 3s and 7s from MNIST dataset it is downloaded as zip file where function unzip it . path = untar_data(URLs.MNIST_SAMPLE) . path . Path(&#39;/home/hassan/.fastai/data/mnist_sample&#39;) . Point to Current Directory to Path . Path.BASE_PATH = path . Lets see whats inside Path . ls is special funtion which returns name of folder inside the path and number of folders as #x where x is count . path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() . sevens . (#6265) [Path(&#39;train/7/10002.png&#39;),Path(&#39;train/7/1001.png&#39;),Path(&#39;train/7/10014.png&#39;),Path(&#39;train/7/10019.png&#39;),Path(&#39;train/7/10039.png&#39;),Path(&#39;train/7/10046.png&#39;),Path(&#39;train/7/10050.png&#39;),Path(&#39;train/7/10063.png&#39;),Path(&#39;train/7/10077.png&#39;),Path(&#39;train/7/10086.png&#39;)...] . Differnet methods to See Pic . Picture is actually represented as matrix of 3 dimension i.e (x,y,z) where z=1 or null in case of grey scale Usually it is 2 dimention and z=3 in case of RBG (colored image) and x and y are real numbers . Lets do following . Read/Get a path of image | Pass it to Image opener by python library | im3_path = threes[1] im3 = Image.open(im3_path) im3 . See image as Array . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . Tensors are Array in Pytorch . The 4:10 indicates we requested the rows from index 4 (included) to 10 (not included) . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . Lets see image in a fancy way . Tensor casted as python data frame and style property allow us to visualise it in a good way | pd.DataFrame(tensor(im3)[4:15,3:22]).style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . Images as Tensor List . Iterate through all paths of images 3/7 | open it as an images | cast it to tensor | Store it as list | sevens_list = [tensor(Image.open(o)) for o in sevens] threes_list = [tensor(Image.open(o)) for o in threes] . type(sevens_list), type(sevens_list[0]) . (list, torch.Tensor) . show_image(sevens_list[0]) . &lt;AxesSubplot:&gt; . Lets Store it as Tensor Stack . sevens_stack = torch.stack(sevens_list).float()/255 threes_stack = torch.stack(threes_list).float()/255 . type(sevens_stack) . torch.Tensor . Compare the types above . type(sevens_list), type(sevens_list[0]) . (list, torch.Tensor) . Put All things Together for Validation . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Prepare Training Data . type([sevens_stack, threes_stack]) ,len([sevens_stack, threes_stack]) . (list, 2) . Lets Reshape images to Dimenstion 1 . train_x = torch.cat([sevens_stack, threes_stack]).view(-1,28*28) . type(train_x),type(train_x[0]), train_x.shape . (torch.Tensor, torch.Tensor, torch.Size([12396, 784])) . Prepare Labels . I want to have all sevens as 0 | all threes as 1 | with Unsequeeze I can have Row vector See what is row vector in google | train_y = tensor([0]*len(sevens_stack) + [1]*len(threes_stack)).unsqueeze(1) . Pytorch Data Set . A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality: . dset = list(zip(train_x,train_y)) . x,y = dset[0] . x.shape,y.shape . (torch.Size([784]), torch.Size([1])) . Prepare Testing Data . Same as training data preperation . valid_x = torch.cat([valid_7_tens ,valid_3_tens]).view(-1,28*28) valid_y = tensor([0]*len(valid_7_tens )+ [1]* len(valid_3_tens)).unsqueeze(1) valid_dset=list(zip(valid_x,valid_y)) . valid_x.shape,valid_y.shape . (torch.Size([2038, 784]), torch.Size([2038, 1])) . Data Loaders : Pytorch API . Lets Explore what is it . Lets see capability of batching -mini batch | dl = DataLoader(range(15), batch_size=5,shuffle=True) . list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . ds=DataLoader(L(enumerate(string.ascii_lowercase)), batch_size=5, shuffle=True) . list(ds) . [(tensor([17, 18, 10, 22, 8]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;)), (tensor([14, 20, 15, 9, 13]), (&#39;o&#39;, &#39;u&#39;, &#39;p&#39;, &#39;j&#39;, &#39;n&#39;)), (tensor([21, 12, 7, 25, 6]), (&#39;v&#39;, &#39;m&#39;, &#39;h&#39;, &#39;z&#39;, &#39;g&#39;)), (tensor([ 5, 11, 23, 1, 3]), (&#39;f&#39;, &#39;l&#39;, &#39;x&#39;, &#39;b&#39;, &#39;d&#39;)), (tensor([ 0, 24, 19, 16, 2]), (&#39;a&#39;, &#39;y&#39;, &#39;t&#39;, &#39;q&#39;, &#39;c&#39;)), (tensor([4]), (&#39;e&#39;,))] . DataLoaders . A DataLoader can be created from a Dataset . dl = DataLoader(dset, batch_size=256,shuffle=True) xb, yb = first(dl) . xb.shape, yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . type(valid_dset),len(valid_dset[0]) . (list, 2) . valid_dl = DataLoader(valid_dset, batch_size=256,shuffle=True) v_xb, v_yb = first(valid_dl) . Initialise Parameters . Lets create a function pass parameters to which we require gradient | weights and biases are are to be initialise | def init_params(size,std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . bias = init_params(1) . Forward Pass . matrix multiplication plus bias . def linear1(x): return x@weights +bias . First Batch . Lets look at this First function take a first minibatch created by data loaders . xb, yb = first(dl) . preds = linear1(train_x) . preds=linear1(xb) . yb.shape,train_y.shape . (torch.Size([256, 1]), torch.Size([12396, 1])) . train_y . tensor([[0], [0], [0], ..., [1], [1], [1]]) . yb . tensor([[0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0]]) . . Step Function as an Activation . Here we have step function as activation Predictions which are greater than 0.5 are considered one class another is second . corrects =(preds&gt;0.5).float() == yb . . corrects.float().mean().item() . 0.40625 . Loss Calculations . since we want to restrict our prediction to range [0-1] as we have labels 0 for seven class and 1 for three class . def mnist_loss(predictions,targets): predictions = predictions.sigmoid() return torch.where(targets==1, (1-predictions), predictions).mean() . loss = mnist_loss(corrects,yb) . Claculate Gradient . take predictions | calculate loss | derivative of loss w.r.t parameters : weights and bias | def calc_grad(x,y,model): preds = model(x) loss = mnist_loss(preds,y) loss.backward() . calc_grad(xb,yb,linear1) . weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0013), tensor([-0.0049])) . Why Gradzero? . Our only remaining step is to update the weights and biases based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too—otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here&#39;s our basic training loop for an epoch . weights.grad.zero_(),bias.grad.zero_(); . def train_epoch(model,lr,params): for x,y in dl: calc_grad(x,y,model) for p in params: p.data -= p.grad*lr p.grad.zero_() . batch accuracy used to evaluate model . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(xb), yb) . tensor(0.4102) . def validate_epoch(model): accs = [batch_accuracy(model(v_xb), v_yb) for v_xb,v_yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.4655 . Lets Start out Training . learning rate here 10 to power -1 | validate used to evaluate model on validation data | lr = 1e-1 params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.5633 . for i in range(400): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.7051 0.7993 0.8414 0.8735 0.8907 0.9004 0.9122 0.9205 0.9275 0.9319 0.9362 0.9367 0.9373 0.9396 0.9417 0.9431 0.9445 0.9445 0.9465 0.9466 0.9494 0.9499 0.9509 0.9519 0.9545 0.9559 0.9559 0.9559 0.9564 0.9572 0.9587 0.9588 0.9597 0.9607 0.9608 0.9627 0.9627 0.9632 0.9637 0.9642 0.9642 0.9641 0.9646 0.9647 0.9651 0.9657 0.9656 0.9661 0.9657 0.9661 0.9667 0.9667 0.9667 0.9666 0.9676 0.9675 0.9676 0.9681 0.9686 0.9686 0.9686 0.9691 0.9692 0.9691 0.969 0.9695 0.9695 0.97 0.9701 0.9701 0.97 0.9701 0.9701 0.9701 0.971 0.9711 0.9716 0.9715 0.9721 0.972 0.972 0.9726 0.9726 0.9725 0.9726 0.9726 0.9725 0.9725 0.9725 0.9726 0.9726 0.9723 0.9725 0.9736 0.9735 0.974 0.974 0.974 0.9745 0.9745 0.9745 0.9745 0.9749 0.975 0.975 0.975 0.975 0.975 0.975 0.975 0.975 0.975 0.975 0.9749 0.975 0.975 0.975 0.975 0.975 0.975 0.975 0.975 0.9754 0.9754 0.9755 0.9754 0.9755 0.9754 0.9755 0.9755 0.976 0.9765 0.9765 0.9765 0.9765 0.9764 0.9765 0.9764 0.9764 0.9764 0.9765 0.9765 0.9765 0.9764 0.9765 0.9765 0.9764 0.9765 0.9765 0.9764 0.9764 0.9764 0.9765 0.9763 0.9764 0.9764 0.9764 0.9764 0.9765 0.9765 0.9765 0.9765 0.9765 0.9764 0.9764 0.9765 0.9765 0.9764 0.9765 0.9765 0.9765 0.9765 0.9765 0.9764 0.9765 0.9765 0.9769 0.977 0.977 0.9769 0.977 0.977 0.977 0.977 0.9769 0.977 0.977 0.9769 0.977 0.9769 0.977 0.9775 0.9774 0.9775 0.9775 0.9774 0.9774 0.9773 0.9774 0.9774 0.9775 0.9774 0.9775 0.9773 0.9774 0.9774 0.9774 0.9775 0.9775 0.9774 0.9774 0.9774 0.9774 0.9774 0.9774 0.9774 0.9775 0.9775 0.9774 0.9774 0.9775 0.9779 0.9779 0.9778 0.9779 0.9779 0.9778 0.9779 0.9779 0.9779 0.9779 0.9784 0.9784 0.9785 0.9784 0.9784 0.9784 0.9784 0.9785 0.9783 0.9783 0.9784 0.9789 0.9789 0.9789 0.9789 0.9789 0.9789 0.979 0.9788 0.979 0.9789 0.9789 0.9789 0.9789 0.9789 0.9789 0.979 0.9789 0.9789 0.9789 0.9789 0.9789 0.9788 0.979 0.9789 0.9789 0.9789 0.9788 0.9788 0.979 0.9789 0.979 0.9789 0.9789 0.9789 0.9788 0.9789 0.9789 0.9794 0.9794 0.9793 0.9795 0.9794 0.9794 0.9793 0.9794 0.9794 0.9794 0.9795 0.9795 0.9794 0.9794 0.9794 0.9794 0.9794 0.9794 0.9794 0.9794 0.9799 0.9798 0.9799 0.9798 0.98 0.9799 0.9799 0.9799 0.9798 0.9799 0.9798 0.9799 0.9799 0.9799 0.9798 0.9799 0.9799 0.9799 0.9799 0.9799 0.9798 0.9799 0.9799 0.9799 0.9799 0.9799 0.9799 0.9799 0.9799 0.9799 0.9799 0.9799 0.9798 0.9798 0.9799 0.9798 0.9799 0.9799 0.9799 0.9799 0.9798 0.9799 0.9799 0.9799 0.9799 0.9798 0.9799 0.9799 0.9798 0.9799 0.9799 0.9799 0.9799 0.9799 0.9798 0.9799 0.9799 0.9799 0.9798 0.9799 0.9798 0.9799 0.9799 0.9799 0.9799 0.9798 0.9799 0.9799 0.9798 0.9799 0.9798 0.9799 0.9799 0.9799 0.9798 0.9798 0.9799 0.9799 0.9799 0.9799 0.9803 0.9804 0.9803 0.9804 0.9803 0.9804 0.9804 0.9804 0.9804 0.9804 0.9804 0.9804 0.9804 0.9804 0.9803 0.9803 0.9803 0.9804 0.9804 0.9804 0.9804 . . Using Python Class . Using same &#39;nn.linear&#39; module inherits object pytorch class &#39;nn.Linear&#39; . nn.Linear does the same thing as our init_params and linear together. It contains both the weights and biases in a single class. Here&#39;s how we replicate our model from the previous section: . linear_model =nn.Linear(28*28,1) . w,b = linear_model.parameters() w.shape, b.shape . (torch.Size([1, 784]), torch.Size([1])) . class BasicOpt: #class contains two attributes params=weights,bias and learning rate # get all the parameters def __init__(self, params,lr): self.params, self.lr = list(params),lr # gradient def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr # zero grad def zero_grad(self, *args , **kwargs): for p in self.params: p.grad = None . opt = BasicOpt(linear_model.parameters(), lr=1e-1) . def train_epoch(model): for x,y in dl: calc_grad(x,y,model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.6556 . def train_model(model,epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(linear_model),end=&quot; &quot; ) . train_model(model=linear_model, epochs = 10) . 0.9657 0.9672 0.9676 0.9686 0.9695 0.9706 0.9706 0.9711 0.9716 0.972 . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr =1e-1) train_model(linear_model,20) . 0.9651 0.9657 0.9687 0.9701 0.9695 0.9705 0.9706 0.971 0.9721 0.973 0.9729 0.973 0.973 0.974 0.9745 0.975 0.9755 0.9754 0.9754 0.9754 . dls = DataLoaders(dl, valid_dl) . To create a Learner without using an application (such as cnn_learner) we need to pass in all the elements that we&#39;ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print: . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . Now we can call fit: . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.154314 | 0.093394 | 0.967125 | 00:00 | . 1 | 0.097860 | 0.069815 | 0.967615 | 00:00 | . 2 | 0.074141 | 0.059789 | 0.969578 | 00:00 | . 3 | 0.062189 | 0.054248 | 0.970069 | 00:00 | . 4 | 0.054846 | 0.050518 | 0.970069 | 00:00 | . 5 | 0.049594 | 0.047826 | 0.970559 | 00:00 | . 6 | 0.045887 | 0.045634 | 0.970559 | 00:00 | . 7 | 0.043330 | 0.044017 | 0.970559 | 00:00 | . 8 | 0.041645 | 0.042539 | 0.972031 | 00:00 | . 9 | 0.039802 | 0.041379 | 0.972031 | 00:00 | .",
            "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/jupyter/2021/09/03/SDG-4.html",
            "relUrl": "/jupyter/2021/09/03/SDG-4.html",
            "date": " • Sep 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural Network From Scratch",
            "content": "Download Dataset . . path = untar_data(URLs.MNIST_SAMPLE) . path . Path(&#39;/home/hassan/.fastai/data/mnist_sample&#39;) . Point to Current Directory This is on Ram . Path.BASE_PATH = path . Return Number of Counts . path.ls() . (#3) [Path(&#39;train&#39;),Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() . sevens . (#6265) [Path(&#39;train/7/10002.png&#39;),Path(&#39;train/7/1001.png&#39;),Path(&#39;train/7/10014.png&#39;),Path(&#39;train/7/10019.png&#39;),Path(&#39;train/7/10039.png&#39;),Path(&#39;train/7/10046.png&#39;),Path(&#39;train/7/10050.png&#39;),Path(&#39;train/7/10063.png&#39;),Path(&#39;train/7/10077.png&#39;),Path(&#39;train/7/10086.png&#39;)...] . Differnet methods to see pic . im3_path = threes[1] im3 = Image.open(im3_path) im3 . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . pd.DataFrame(tensor(im3)[4:15,3:22]).style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . sevens_list = [tensor(Image.open(o)) for o in sevens] threes_list = [tensor(Image.open(o)) for o in threes] . type(sevens_list), type(sevens_list[0]) . (list, torch.Tensor) . show_image(sevens_list[0]) . &lt;AxesSubplot:&gt; . Lets Stack as Tensor . sevens_stack = torch.stack(sevens_list).float()/255 threes_stack = torch.stack(threes_list).float()/255 . type(sevens_stack) . torch.Tensor . Compare the types above . type(sevens_list), type(sevens_list[0]) . (list, torch.Tensor) . Put All things Together for Validation . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Prepare Training Data . type([sevens_stack, threes_stack]) ,len([sevens_stack, threes_stack]) . (list, 2) . train_x = torch.cat([sevens_stack, threes_stack]).view(-1,28*28) . type(train_x),type(train_x[0]), train_x.shape . (torch.Tensor, torch.Tensor, torch.Size([12396, 784])) . train_y = tensor([0]*len(sevens_stack) + [1]*len(threes_stack)).unsqueeze(1) . . dset = list(zip(train_x,train_y)) . x,y = dset[0] . x.shape,y.shape . (torch.Size([784]), torch.Size([1])) . Prepare Testing Data . valid_x = torch.cat([valid_7_tens ,valid_3_tens]).view(-1,28*28) valid_y = tensor([0]*len(valid_7_tens )+ [1]* len(valid_3_tens)).unsqueeze(1) valid_dset=list(zip(valid_x,valid_y)) . valid_x.shape,valid_y.shape . (torch.Size([2038, 784]), torch.Size([2038, 1])) . Data Loaders : Pytorch API . dl = DataLoader(range(15), batch_size=5,shuffle=True) . list(dl) . [tensor([ 3, 12, 8, 10, 2]), tensor([ 9, 4, 7, 14, 5]), tensor([ 1, 13, 0, 6, 11])] . ds=DataLoader(L(enumerate(string.ascii_lowercase)), batch_size=5, shuffle=True) . list(ds) . [(tensor([17, 18, 10, 22, 8]), (&#39;r&#39;, &#39;s&#39;, &#39;k&#39;, &#39;w&#39;, &#39;i&#39;)), (tensor([14, 20, 15, 9, 13]), (&#39;o&#39;, &#39;u&#39;, &#39;p&#39;, &#39;j&#39;, &#39;n&#39;)), (tensor([21, 12, 7, 25, 6]), (&#39;v&#39;, &#39;m&#39;, &#39;h&#39;, &#39;z&#39;, &#39;g&#39;)), (tensor([ 5, 11, 23, 1, 3]), (&#39;f&#39;, &#39;l&#39;, &#39;x&#39;, &#39;b&#39;, &#39;d&#39;)), (tensor([ 0, 24, 19, 16, 2]), (&#39;a&#39;, &#39;y&#39;, &#39;t&#39;, &#39;q&#39;, &#39;c&#39;)), (tensor([4]), (&#39;e&#39;,))] . dl = DataLoader(dset, batch_size=256,shuffle=True) xb, yb = first(dl) . xb.shape, yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . type(valid_dset),len(valid_dset[0]) . (list, 2) . valid_dl = DataLoader(valid_dset, batch_size=256,shuffle=True) v_xb, v_yb = first(valid_dl) . Initialise Parameters . def init_params(size,std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . bias = init_params(1) . Forward Pass . def linear1(x): return x@weights +bias . Lets see the first Batch . xb, yb = first(dl) . preds = linear1(train_x) . preds=linear1(xb) . yb.shape,train_y.shape . (torch.Size([256, 1]), torch.Size([12396, 1])) . train_y . tensor([[0], [0], [0], ..., [1], [1], [1]]) . yb . tensor([[0], [0], [1], [1], [0], [1], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [0], [0], [0], [1], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [0], [1], [0], [0], [0], [0], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [0], [0]]) . Step Function as an Activation . corrects =(preds&gt;0.5).float() == yb . . corrects.float().mean().item() . 0.3203125 . LOSS CALCULATION . def mnist_loss(predictions,targets): predictions = predictions.sigmoid() return torch.where(targets==1, (1-predictions), predictions).mean() . loss = mnist_loss(corrects,yb) . GRADIENT . def calc_grad(x,y,model): preds = model(x) loss = mnist_loss(preds,y) loss.backward() . calc_grad(xb,yb,linear1) . weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0029), tensor([-0.0199])) . weights.grad.zero_(),bias.grad.zero_(); . def train_epoch(model,lr,params): for x,y in dl: calc_grad(x,y,model) for p in params: p.data -= p.grad*lr p.grad.zero_() . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(xb), yb) . tensor(0.3242) . def validate_epoch(model): accs = [batch_accuracy(model(v_xb), v_yb) for v_xb,v_yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.3109 . lr = 1e-1 params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.4096 . for i in range(400): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.4563 0.4724 0.4789 0.4832 0.4868 0.4882 0.4892 0.4899 0.4911 0.4921 0.4924 0.4919 0.4926 0.4933 0.4941 0.494 0.4945 0.4949 0.4947 0.4953 0.4951 0.4949 0.4951 0.4951 0.4952 0.4955 0.4951 0.4951 0.4951 0.4952 0.4951 0.4948 0.4948 0.4951 0.4955 0.4957 0.4957 0.4955 0.4958 0.4956 0.4957 0.4955 0.4959 0.4958 0.4956 0.496 0.4956 0.4955 0.4956 0.4955 0.4957 0.4953 0.4957 0.4957 0.4959 0.4956 0.4955 0.4958 0.4955 0.4955 0.4955 0.4955 0.4955 0.4954 0.4956 0.4954 0.4956 0.4954 0.4954 0.4957 0.4957 0.4955 0.4954 0.4954 0.4956 0.4955 0.4957 0.4957 0.4957 0.4955 0.4957 0.4956 0.4956 0.4957 0.4956 0.4956 0.4954 0.4957 0.4954 0.4956 0.4957 0.4954 0.4958 0.4957 0.4957 0.4956 0.4956 0.4956 0.4954 0.4956 0.4955 0.4955 0.4956 0.4953 0.4955 0.4957 0.4958 0.4957 0.4953 0.4955 0.4955 0.4957 0.4957 0.4955 0.4956 0.4956 0.4958 0.4957 0.4955 0.4956 0.4957 0.4956 0.4955 0.4955 0.4953 0.4953 0.4954 0.4955 0.4956 0.4956 0.4957 0.4956 0.4956 0.4953 0.4956 0.4957 0.4956 0.4958 0.4953 0.4956 0.4956 0.4956 0.4956 0.4955 0.4955 0.4956 0.4956 0.4956 0.4954 0.4954 0.4954 0.4955 0.4957 0.4958 0.4954 0.4951 0.4955 0.4955 0.4957 0.4953 0.4958 0.4957 0.4956 0.4956 0.4957 0.4957 0.496 0.4962 0.4962 0.4957 0.4961 0.4961 0.496 0.4961 0.4963 0.4961 0.4961 0.496 0.4961 0.4962 0.496 0.4961 0.4963 0.4962 0.4961 0.4961 0.4962 0.4959 0.4964 0.4963 0.4963 0.4958 0.4962 0.496 0.4961 0.4961 0.496 0.4958 0.496 0.496 0.4962 0.4962 0.4961 0.4959 0.4962 0.4959 0.4961 0.4961 0.496 0.4961 0.4964 0.4964 0.4962 0.4964 0.4961 0.4961 0.4958 0.496 0.4962 0.4962 0.4956 0.4962 0.4962 0.4964 0.4961 0.4961 0.4963 0.4959 0.4962 0.4963 0.4957 0.4959 0.4962 0.4962 0.496 0.4961 0.4961 0.496 0.496 0.4966 0.4969 0.4969 0.4969 0.4965 0.4978 0.4979 0.4985 0.4997 0.5023 0.5037 0.5099 0.523 0.5494 0.6028 0.736 0.8277 0.8562 0.8794 0.8885 0.901 0.9034 0.9131 0.9196 0.9243 0.9279 0.9289 0.9318 0.9353 0.9361 0.9371 0.9396 0.9402 0.9407 0.9421 0.9444 0.9455 0.946 0.9476 0.9499 0.9515 0.9525 0.9523 0.9535 0.9539 0.9544 0.9544 0.9544 0.9548 0.9563 0.9573 0.9579 0.9578 0.9583 0.9583 0.9584 0.9588 0.9588 0.9593 0.9592 0.9593 0.9592 0.9593 0.9592 0.9592 0.9598 0.9598 0.9603 0.9601 0.9598 0.9597 0.9603 0.9602 0.9602 0.9607 0.9607 0.9607 0.9607 0.9603 0.9602 0.9603 0.9602 0.9602 0.9606 0.9607 0.9607 0.9608 0.9608 0.9608 0.9608 0.9612 0.9617 0.9617 0.9617 0.9618 0.9616 0.9623 0.9623 0.9627 0.9627 0.9626 0.9632 0.9632 0.9633 0.9632 0.9632 0.9632 0.9632 0.9631 0.9632 0.9632 0.9633 0.9632 0.9632 0.9632 0.9632 0.9637 0.9637 0.9636 0.9632 0.9632 0.9631 0.9632 0.9632 0.9632 0.9641 0.9642 0.9647 0.9646 0.9647 0.9647 0.9662 0.966 0.9661 0.966 0.9666 0.9667 0.9671 0.9675 0.9677 0.9676 0.9681 0.9681 0.9681 0.9681 0.9686 0.9687 0.9686 0.9687 0.9686 0.9686 0.9686 0.9686 0.9686 0.969 0.9691 0.969 0.9691 0.9691 0.9692 0.9691 .",
            "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/2021/08/29/SDG-3.html",
            "relUrl": "/2021/08/29/SDG-3.html",
            "date": " • Aug 29, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "3. Breed Classification",
            "content": "3. Neural Network . A tutorial for beginners with Pytorch and FASTAI you can create your own classifier. . toc:true - badges: true | comments: true | author: HAFIZ AHMAD HASSAN &amp; Jeremy Howard | categories: [jupyter] | image: images/chart-preview.png | . This tutorial is created from Lecture 4 from FAST ai Course Deep Learning from coders Course I will go through step by step how to build Classifier using pytorch from scratch. . Image Classification . path = untar_data(URLs.PETS) . Why we are USIGN BASE_PATH . we want to nicely represent our data paths relative to our current path Look at path.ls() . Path.BASE_PATH = path . path.ls() . (#4) [Path(&#39;annotations&#39;),Path(&#39;images&#39;),Path(&#39;models&#39;),Path(&#39;crappy&#39;)] . (path/&quot;images&quot;).ls() . (#7394) [Path(&#39;images/Sphynx_245.jpg&#39;),Path(&#39;images/miniature_pinscher_55.jpg&#39;),Path(&#39;images/havanese_20.jpg&#39;),Path(&#39;images/miniature_pinscher_34.jpg&#39;),Path(&#39;images/samoyed_91.jpg&#39;),Path(&#39;images/chihuahua_123.jpg&#39;),Path(&#39;images/yorkshire_terrier_155.jpg&#39;),Path(&#39;images/Egyptian_Mau_79.jpg&#39;),Path(&#39;images/scottish_terrier_23.jpg&#39;),Path(&#39;images/basset_hound_198.jpg&#39;)...] . Remember . Most of function we are using in fastai are belong to Class &quot;L&quot; instead of list Ehanced list ( showing number of items , more items are denoted as &quot;..&quot; . Last time first letter is capital then cat otherwise dog . here our case is different . Regular expression help us to get labels . Please google re if you havnt gone through . There is FASTai NLP course a--2 regix lessons . Bit hard to get sometimes . Lets Pick file name and see how it is . fname = (path/&quot;images&quot;).ls()[0] . fname.name . &#39;Sphynx_245.jpg&#39; . Little Experiment With RE ( Regular Expression) . re is module | findall grab all parts of regular expression | that have parantheses around them | r is special kind string which says dont treat backslashes special remember in python backslashes is newline | &#39;r(.+) d.jpg &#39;-- means string pick any &quot;.&quot; letter &quot;+&quot; can be repeated one or more time which is followed by under score &quot;&quot; &quot; d+&quot; followed by digit one or more time (&quot;.&quot; --followed by anything can be . ) followed by &quot;jpg&quot; (&quot;dollar&quot; followed by end of string) | re.findall(r&#39;(.+)_ d+.jpg$&#39;,fname.name) . [&#39;Sphynx&#39;] . DataBlock . Now we blocks expect dependent and independent variable | get items --get images files | splitter- Random splitt data | get_y --using attribute which takes Regex LAbler function which will be passed to attribute &quot;name&quot; | aug transform we saw in lesson 2 section aug transformer .. its basically synthetic | Resize to very large image 460 then using aug trans to have smaller size | why? . this is called Presizing . details are below Steps . resize grab square randomly if its portrait then grab randomly full width grab random from top to bottom | secondly augmernt transform resize grab random wraped crop possibly rotated and turn that into square (rotation ,wrapping ,zooming) to smaller to 224 by 224 | note : first step turning square. but seccond step can happen in gpu normally things like rotating and cropping are pretty slow . (rotation ,wrapping ,zooming) are actually desruptive to image becasue each one requires interpolation step which not just slow but makes images low quality . whats unique in fast ai . we are keeping track of changing. coordinate values in non-lossy way ,so the full floting point value and then once at very end we will do interpolation . look taddy bears . left - presizing approach right - using python libraries . there are wierd things over here Flaws . less nicely focused | grass | distortion on leg sides | Details Presizing . We need our images to have the same dimensions, so that they can collate into tensors to be passed to the GPU. We also want to minimize the number of distinct augmentation computations we perform. The performance requirement suggests that we should, where possible, compose our augmentation transforms into fewer transforms (to reduce the number of computations and the number of lossy operations) and transform the images into uniform sizes (for more efficient processing on the GPU). . The challenge is that, if performed after resizing down to the augmented size, various common data augmentation transforms might introduce spurious empty zones, degrade data, or both. For instance, rotating an image by 45 degrees fills corner regions of the new bounds with emptiness, which will not teach the model anything. Many rotation and zooming operations will require interpolating to create pixels. These interpolated pixels are derived from the original image data but are still of lower quality. . To work around these challenges, presizing adopts two strategies that are shown in &lt;&gt;:&lt;/p&gt; Resize images to relatively &quot;large&quot; dimensions—that is, dimensions significantly larger than the target training dimensions. | Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times. | The first step, the resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller. . In the second step, the GPU is used for all data augmentation, and all of the potentially destructive operations are done together, with a single interpolation at the end. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &quot;&quot;&quot; &lt;img alt=&quot;Presizing on the training set&quot; width=&quot;600&quot; caption=&quot;Presizing on the training set&quot; id=&quot;presizing&quot; src=&quot;images/att_00060.png&quot;&gt; &quot;&quot;&quot; . &#39; n&lt;img alt=&#34;Presizing on the training set&#34; width=&#34;600&#34; caption=&#34;Presizing on the training set&#34; id=&#34;presizing&#34; src=&#34;images/att_00060.png&#34;&gt; n&#39; . This picture shows the two steps: . Crop full width or height: This is in item_tfms, so it&#39;s applied to each individual image before it is copied to the GPU. It&#39;s used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen. | Random crop and augment: This is in batch_tfms, so it&#39;s applied to a batch all at once on the GPU, which means it&#39;s fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first. | To implement this process in fastai you use Resize as an item transform with a large size, and RandomResizedCrop as a batch transform with a smaller size. RandomResizedCrop will be added for you if you include the min_scale parameter in your aug_transforms function, as was done in the DataBlock call in the previous section. Alternatively, you can use pad or squish instead of crop (the default) for the initial Resize. . &lt;&gt; shows the difference between an image that has been zoomed, interpolated, rotated, and then interpolated again (which is the approach used by all other deep learning libraries), shown here on the right, and an image that has been zoomed and rotated as one operation and then interpolated just once on the left (the fastai approach), shown here on the left.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . pets = DataBlock( blocks =(ImageBlock, CategoryBlock), # ordered list get_items = get_image_files, splitter= RandomSplitter(seed=42), get_y = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;),&#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) . dls = pets.dataloaders(path/&quot;images&quot;) . Lets Debug DataLoader . show batch is for each mini batch it will show data if loaded properly . . Lets Debug Augmentation . get unique = &quot;true&quot; . . Failure in DataBlock . Issues . different images different sizes | unable to collate them to batch | you can see everything happens . #pets1.summary(path/&quot;images&quot;) . Question . What if your image size is less than resize? . Ans: if you remember lesson we look at different ways to create this thing . squish . Pad . etc . Squish and Pad will help you . You model can teach you about problem is your data . we are getting 7 percent error . learn = cnn_learner(dls,resnet34, metrics= error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.499157 | 0.335323 | 0.113667 | 00:42 | . epoch train_loss valid_loss error_rate time . 0 | 0.497216 | 0.289698 | 0.098106 | 00:52 | . 1 | 0.333692 | 0.221010 | 0.076455 | 00:53 | . Train Model help Clean Data . why? . Initial model will help you clean data . Remember we have . interpret.toplosses help us identify mislables . confusion matrix help us where we are confused . ImageClassifierCleaner let us find for example two bears top confused things . Model helping you and then go ahead train data after cleaning . Notebook4 included loss function Fastai atomatically pick good loss function . Lets look what acc. it picks . learn.loss_func . FlattenedLoss of CrossEntropyLoss() . Cross Entropy . Same as Mnist loss ..kind of extended version . torch.where only works when you have binary outcome . we want to create just like that but we want to make it work more than two categories . Lets see whats inside Batch . destructure batch size = 64 . dls.vocab . x,y = dls.one_batch() . y . TensorCategory([ 0, 25, 33, 19, 12, 22, 35, 17, 23, 32, 8, 5, 3, 33, 36, 34, 32, 7, 1, 14, 5, 14, 8, 36, 18, 13, 22, 5, 0, 20, 18, 33, 28, 28, 19, 33, 26, 0, 30, 25, 27, 23, 31, 1, 17, 13, 8, 23, 34, 24, 28, 7, 13, 12, 31, 10, 29, 33, 22, 0, 21, 20, 3, 6], device=&#39;cuda:0&#39;) . dls.vocab . [&#39;Abyssinian&#39;, &#39;Bengal&#39;, &#39;Birman&#39;, &#39;Bombay&#39;, &#39;British_Shorthair&#39;, &#39;Egyptian_Mau&#39;, &#39;Maine_Coon&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;Russian_Blue&#39;, &#39;Siamese&#39;, &#39;Sphynx&#39;, &#39;american_bulldog&#39;, &#39;american_pit_bull_terrier&#39;, &#39;basset_hound&#39;, &#39;beagle&#39;, &#39;boxer&#39;, &#39;chihuahua&#39;, &#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, &#39;german_shorthaired&#39;, &#39;great_pyrenees&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;keeshond&#39;, &#39;leonberger&#39;, &#39;miniature_pinscher&#39;, &#39;newfoundland&#39;, &#39;pomeranian&#39;, &#39;pug&#39;, &#39;saint_bernard&#39;, &#39;samoyed&#39;, &#39;scottish_terrier&#39;, &#39;shiba_inu&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;wheaten_terrier&#39;, &#39;yorkshire_terrier&#39;] . dls.vocab[16] . &#39;boxer&#39; . View the predictions . its just call the last activation . preds,_ = learn.get_preds(dl= [(x,y)]) preds[0] . tensor([9.5199e-01, 1.0111e-02, 6.5786e-05, 2.1975e-04, 7.8775e-04, 3.6360e-03, 3.0038e-03, 1.0312e-04, 2.4044e-02, 5.6874e-04, 5.1095e-05, 3.3239e-03, 2.5059e-05, 5.7563e-06, 1.0179e-05, 9.1572e-06, 8.6417e-06, 2.3385e-04, 1.0258e-05, 1.3129e-05, 7.4432e-06, 8.3646e-06, 5.2068e-05, 1.6303e-05, 4.9455e-06, 6.5756e-06, 6.5956e-05, 9.1565e-06, 3.6239e-05, 1.1460e-05, 1.4704e-05, 3.3519e-05, 9.6280e-04, 3.4956e-04, 7.9019e-07, 1.6408e-05, 1.7885e-04]) . len(preds[0]), preds[0].sum() . (37, tensor(1.0000)) . How do we go about this prediction . Softmax is an extension of sigmoid handle more than two categoreis . what if we want 37 cat. . we need one activation for 1 category e.g in case 3,7 activations are two . below 1st column is activation of 1st cat and 2nd is for 7 . like how much like 3 and how much like 7 . torch.random.manual_seed(42), acts = torch.randn((6,2))*2 acts #How much likely is first and how muc to 7 i.e . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . Taking Sigmoid . if we take it values will be between 0 or 1 . but dont add up to one . so doesnt make sense . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . Solution : . So if we take difference . | Relative confidence: take sigmoid after . | diff = acts[:,0]- acts[:,1] . diff . tensor([ 0.4158, 0.0083, -1.8731, 5.6924, 0.3886, -0.5489]) . torch.stack([diff.sigmoid(),1-diff.sigmoid()],dim=1) . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . More than 2 cat: . Use Softmax . in binary case it is equal to sigmoid . The second column (the probability of it being a 7) will then just be that value subtracted from 1. Now, we need a way to do all this that also works for more than two columns. It turns out that this function, called softmax, is exactly that: . def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True) . jargon:Exponential function (exp): Literally defined as e**x, where e is a special number approximately equal to 2.718. It is the inverse of the natural logarithm function. Note that exp is always positive, and it increases very rapidly! . Let&#39;s check that softmax returns the same values as sigmoid for the first column, and those values subtracted from 1 for the second column: . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . softmax is the multi-category equivalent of sigmoid—we have to use it any time we have more than two categories and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent. We could create other functions that have the properties that all activations are between 0 and 1, and sum to 1; however, no other function has the same relationship to the sigmoid function, which we&#39;ve seen is smooth and symmetric. Also, we&#39;ll see shortly that the softmax function works well hand-in-hand with the loss function we will look at in the next section. . If we have three output activations, such as in our bear classifier, calculating softmax for a single bear image would then look like something like &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &quot;&quot;&quot; &lt;img alt=&quot;Bear softmax example&quot; width=&quot;280&quot; id=&quot;bear_softmax&quot; caption=&quot;Example of softmax on the bear classifier&quot; src=&quot;images/att_00062.png&quot;&gt; &quot;&quot;&quot; . &#39; n&lt;img alt=&#34;Bear softmax example&#34; width=&#34;280&#34; id=&#34;bear_softmax&#34; caption=&#34;Example of softmax on the bear classifier&#34; src=&#34;images/att_00062.png&#34;&gt; n&#39; . Interesting thing . e to power something grows really fast see below . if we have one activation is bit bigger than other . then softmax is really big . its tries to pick one whcih one . thats not you always want sometimes you have inference time you want to bit concious . its default you do most of time . so that is somtmax . math.exp(4) . 54.598150033144236 . math.exp(6) . math.exp(6) . 403.4287934927351 . Log Likelihood . binary case. we did this . def mnist_loss(inputs, targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).mean() . its fine it worked so we could do thing exactly same thing . because tagets are not 0 or 1 . targ = tensor([0,1,0,1,1,0]) . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Replace torch.where . grab all number from 0-5 | my targets | each row number it will pick particular column defined in target | lets see pick column 0 for first row | so this is super nifty indexing expression you should play with . 1st thing say which row you should return . second says which column . we can use that more than two values . idx = range(6) sm_acts[idx,targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . from IPython.display import HTML df = pd.DataFrame(sm_acts , columns=[&quot;3&quot;,&quot;7&quot;]) df[&quot;target&quot;] = targ df[&quot;idx&quot;] = idx df[&quot;loss&quot;] = sm_acts[range(6), targ] t= df.style.hide_index() #To have html code compatible with our script html = t._repr_html_().split(&#39;&lt;/style&gt;&#39;)[1] html = re.sub(r&#39;&lt;table id=&quot;([^&quot;]+)&quot; s*&gt;&#39;, r&#39;&lt;table &gt;&#39;, html) display(HTML(html)) . 3 7 target idx loss . 0.602469 | 0.397531 | tensor(0) | 0 | tensor(0.6025) | . 0.502065 | 0.497935 | tensor(1) | 1 | tensor(0.4979) | . 0.133188 | 0.866811 | tensor(0) | 2 | tensor(0.1332) | . 0.996640 | 0.003360 | tensor(1) | 3 | tensor(0.0034) | . 0.595949 | 0.404051 | tensor(1) | 4 | tensor(0.4041) | . 0.366118 | 0.633882 | tensor(0) | 5 | tensor(0.3661) | . How to make it Work More than 2 colums . full mnist : we will have more 10 columns | indexer | Negative Log Liklihood . There is no log in it we will see . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts,targ,reduction=&quot;none&quot;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . Lets Talk about Logs . Problem : That means that our model will not care whether it predicts 0.99 or 0.999. Indeed, those numbers are so close together but in another sense, 0.999 is 10 times more confident than 0.99 . The function we saw in the previous section works quite well as a loss function, but we can make it a bit better. The problem is that we are using probabilities, and probabilities cannot be smaller than 0 or greater than 1. That means that our model will not care whether it predicts 0.99 or 0.999. Indeed, those numbers are so close together—but in another sense, 0.999 is 10 times more confident than 0.99. So, we want to transform our numbers between 0 and 1 to instead be between negative infinity and infinnity. There is a mathematical function that does exactly this: the logarithm (available as torch.log). It is not defined for numbers less than 0, and looks like this: . so this log fuctoin . we can acc make it better . what if model decide 0.99 or 0.999 . if we have 1000 things then right one is better than 0.99 . so really we like to transform numbers between 0-1 instead -infinite to positive infinite . log will help us in this case . so . numbers as we closer to zero its goes down to infinity at 1 it goes to zeros . torch.log(tensor(0.01)), torch.log(tensor(1)),torch.log(tensor(0.)) . (tensor(-4.6052), tensor(0.), tensor(-inf)) . we cant go zero . our loss function we want to be negative . (y = b power a) a= log(y,b) . what intersting . log (a*b) = log(a) + log(b) . a*b can be very very big or small . adding not get out of control . when we take the probabilities such as sm_acts . we take log . | we take mean . | that is called negative log likelihood . *if we take softmax and then log . pass to nll_loss . which is cross Entropy Loss . why nll_loss dont take Log? . the reason it is more convinient to take log back at softmax step . so pytorch has fuction. log_softmax since it is very easir pytirch assume u did log softmax and pass to nll loss function . . Two ways for Cross entropy Loss . single number because of mean . reduction = none . for looking all . loss_func=nn.CrossEntropyLoss() . loss_func(acts,targ) . tensor(1.8045) . F.cross_entropy(acts,targ) . tensor(1.8045) . nn.CrossEntropyLoss(reduction=&quot;none&quot;)(acts,targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . why loss function needs to negative? . Lower it is better . needed to cuttoff . next week Data Ethics: . &lt;/div&gt; .",
            "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/jupyter/2021/08/04/blog-2.html",
            "relUrl": "/jupyter/2021/08/04/blog-2.html",
            "date": " • Aug 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hafizahmadhassan.github.io/HafizAhmadHassan/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}