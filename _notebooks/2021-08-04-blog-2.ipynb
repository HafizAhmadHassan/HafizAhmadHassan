{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43637963",
   "metadata": {},
   "source": [
    "# 3. Breed Classification\n",
    "> A tutorial for beginners with Pytorch and FASTAI you can create your own classifier.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: HAFIZ AHMAD HASSAN & Jeremy Howard\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce69e2",
   "metadata": {},
   "source": [
    "# 3. Neural Network \n",
    "> A tutorial for beginners with Pytorch and FASTAI you can create your own classifier.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: HAFIZ AHMAD HASSAN & Jeremy Howard\n",
    "- categories: [jupyter]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b3463",
   "metadata": {},
   "source": [
    "This tutorial is created from Lecture 4 from FAST ai Course Deep Learning from coders Course I will go through step by step how to build Classifier using pytorch from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a22dd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un comment pip install if running colab\n",
    "#!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "\n",
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b32288",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de232dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = untar_data(URLs.PETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4a4ff",
   "metadata": {},
   "source": [
    "## Why we are  USIGN BASE_PATH\n",
    "\n",
    "we want to nicely represent our data paths relative to our current path\n",
    "Look at path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12bd6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "570aef58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#4) [Path('annotations'),Path('images'),Path('models'),Path('crappy')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bda06c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7394) [Path('images/Sphynx_245.jpg'),Path('images/miniature_pinscher_55.jpg'),Path('images/havanese_20.jpg'),Path('images/miniature_pinscher_34.jpg'),Path('images/samoyed_91.jpg'),Path('images/chihuahua_123.jpg'),Path('images/yorkshire_terrier_155.jpg'),Path('images/Egyptian_Mau_79.jpg'),Path('images/scottish_terrier_23.jpg'),Path('images/basset_hound_198.jpg')...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/\"images\").ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af834f20",
   "metadata": {},
   "source": [
    "## Remember\n",
    "\n",
    "Most of function we are using in fastai are belong to Class \"L\" instead of list Ehanced list ( showing number of items , more items are denoted as \"..\"\n",
    "\n",
    "Last time first letter is capital then cat otherwise dog\n",
    "\n",
    "here our case is different\n",
    "\n",
    "Regular expression help us to get labels\n",
    "\n",
    "Please google re if you havnt gone through\n",
    "\n",
    "There is FASTai NLP course a--2 regix lessons\n",
    "\n",
    "\n",
    "Bit hard to get sometimes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7772aeca",
   "metadata": {},
   "source": [
    " **Lets Pick file name and see how it is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fe385f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = (path/\"images\").ls()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ceb5253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sphynx_245.jpg'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306128d",
   "metadata": {},
   "source": [
    "## Little Experiment With RE ( Regular Expression)\n",
    "\n",
    "1. re is module\n",
    "2. findall grab all parts of regular expression\n",
    "3. that have parantheses around them\n",
    "4. r is special kind string which says dont treat backslashes\\ special remember in python backslashes is newline\n",
    "5. 'r(.+)_\\d.jpg '-- means string pick any \".\" letter \"+\" can be repeated one or more time which is followed by under score \"_\" \"\\d+\" followed by digit one or more time (\".\" --followed by anything can be \\. ) followed by \"jpg\" (\"dollar\" followed by end of string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c359e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sphynx']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(.+)_\\d+.jpg$',fname.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c3d27",
   "metadata": {},
   "source": [
    "## DataBlock\n",
    "1. Now we blocks expect dependent and independent variable\n",
    "2. get items --get images files\n",
    "3. splitter- Random splitt data\n",
    "4. get_y  --using attribute which takes Regex LAbler function which will be passed to attribute \"name\"\n",
    "5. aug transform we saw in lesson 2 section aug transformer .. its basically synthetic \n",
    "6. Resize to very large image 460 then using aug trans  to have smaller size \n",
    "\n",
    "**why?**\n",
    "\n",
    "## this is called Presizing\n",
    "details are below\n",
    "Steps\n",
    "1. resize grab square randomly if its portrait then grab randomly full width grab random from top to bottom\n",
    "2. secondly augmernt transform resize grab random wraped crop possibly rotated and turn that into square (rotation ,wrapping ,zooming) to smaller to 224 by 224\n",
    "\n",
    "note : first step turning square. but seccond step can happen in gpu normally things like rotating and cropping are pretty slow\n",
    "\n",
    "\n",
    "(rotation ,wrapping ,zooming) are actually desruptive to image becasue each one requires interpolation step which not just slow but makes images low quality\n",
    "\n",
    "\n",
    "\n",
    "**whats unique in fast ai**\n",
    "\n",
    "we are keeping track of changing. coordinate values in non-lossy way ,so the full floting point value and  then once at very end we will do interpolation\n",
    "\n",
    "\n",
    "look taddy bears\n",
    "\n",
    "left - presizing approach\n",
    "right - using python libraries\n",
    "\n",
    "there are wierd things over here\n",
    "Flaws\n",
    "1. less nicely focused\n",
    "2. grass \n",
    "3. distortion on leg sides\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f184ebb",
   "metadata": {},
   "source": [
    "##  Details Presizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fae30b",
   "metadata": {},
   "source": [
    "We need our images to have the same dimensions, so that they can collate into tensors to be passed to the GPU. We also want to minimize the number of distinct augmentation computations we perform. The performance requirement suggests that we should, where possible, compose our augmentation transforms into fewer transforms (to reduce the number of computations and the number of lossy operations) and transform the images into uniform sizes (for more efficient processing on the GPU).\n",
    "\n",
    "The challenge is that, if performed after resizing down to the augmented size, various common data augmentation transforms might introduce spurious empty zones, degrade data, or both. For instance, rotating an image by 45 degrees fills corner regions of the new bounds with emptiness, which will not teach the model anything. Many rotation and zooming operations will require interpolating to create pixels. These interpolated pixels are derived from the original image data but are still of lower quality.\n",
    "\n",
    "To work around these challenges, presizing adopts two strategies that are shown in <<presizing>>:\n",
    "\n",
    "1. Resize images to relatively \"large\" dimensions—that is, dimensions significantly larger than the target training dimensions. \n",
    "1. Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times.\n",
    "\n",
    "The first step, the resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller.\n",
    "\n",
    "In the second step, the GPU is used for all data augmentation, and all of the potentially destructive operations are done together, with a single interpolation at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8620855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<img alt=\"Presizing on the training set\" width=\"600\" caption=\"Presizing on the training set\" id=\"presizing\" src=\"images/att_00060.png\">\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "<img alt=\"Presizing on the training set\" width=\"600\" caption=\"Presizing on the training set\" id=\"presizing\" src=\"images/att_00060.png\">\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd69c2",
   "metadata": {},
   "source": [
    "This picture shows the two steps:\n",
    "\n",
    "1. *Crop full width or height*: This is in `item_tfms`, so it's applied to each individual image before it is copied to the GPU. It's used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.\n",
    "2. *Random crop and augment*: This is in `batch_tfms`, so it's applied to a batch all at once on the GPU, which means it's fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.\n",
    "\n",
    "To implement this process in fastai you use `Resize` as an item transform with a large size, and `RandomResizedCrop` as a batch transform with a smaller size. `RandomResizedCrop` will be added for you if you include the `min_scale` parameter in your `aug_transforms` function, as was done in the `DataBlock` call in the previous section. Alternatively, you can use `pad` or `squish` instead of `crop` (the default) for the initial `Resize`.\n",
    "\n",
    "<<interpolations>> shows the difference between an image that has been zoomed, interpolated, rotated, and then interpolated again (which is the approach used by all other deep learning libraries), shown here on the right, and an image that has been zoomed and rotated as one operation and then interpolated just once on the left (the fastai approach), shown here on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a0374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_image_files??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca76c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = DataBlock( blocks =(ImageBlock, CategoryBlock), # ordered list\n",
    "                get_items = get_image_files,\n",
    "                splitter= RandomSplitter(seed=42),\n",
    "                get_y = using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'),'name'),\n",
    "                item_tfms=Resize(460),\n",
    "                batch_tfms=aug_transforms(size=224, min_scale=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9df02fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = pets.dataloaders(path/\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171496a1",
   "metadata": {},
   "source": [
    "## Lets Debug DataLoader\n",
    "\n",
    "show batch is for each mini batch it will show data if loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "def86373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls.show_batch(nrows=1,ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cf75f",
   "metadata": {},
   "source": [
    "## Lets Debug Augmentation\n",
    "get unique = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7359e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls.show_batch(nrows=1,unique=True,ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2414f",
   "metadata": {},
   "source": [
    "## Failure in DataBlock\n",
    "Issues\n",
    "1. different images different sizes\n",
    "2. unable to collate them to batch\n",
    "\n",
    "\n",
    "you can see everything happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0647141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "pets1 = DataBlock(blocks = (ImageBlock,CategoryBlock),\n",
    "                 get_items= get_image_files,\n",
    "                 splitter=RandomSplitter(seed=42),\n",
    "                 get_y = using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'),\"name\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a22982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see output\n",
    "#pets1.summary(path/\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eccb803",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "What if your image size is less than resize?\n",
    "\n",
    "Ans: if you remember lesson we look at different ways to create this thing\n",
    "\n",
    "**squish**\n",
    "\n",
    "**Pad**\n",
    "\n",
    "etc\n",
    "\n",
    "Squish and Pad will help you\n",
    "\n",
    "You model can teach you about problem is your data\n",
    "\n",
    "\n",
    "we are getting 7 percent error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "41ef04a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.499157</td>\n",
       "      <td>0.335323</td>\n",
       "      <td>0.113667</td>\n",
       "      <td>00:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.497216</td>\n",
       "      <td>0.289698</td>\n",
       "      <td>0.098106</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.333692</td>\n",
       "      <td>0.221010</td>\n",
       "      <td>0.076455</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = cnn_learner(dls,resnet34, metrics= error_rate)\n",
    "learn.fine_tune(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f3a6b",
   "metadata": {},
   "source": [
    "## Train Model help Clean Data\n",
    "\n",
    "why? \n",
    "\n",
    "Initial model will help you clean data \n",
    "\n",
    "Remember we have \n",
    "\n",
    "**interpret.toplosses** help us identify mislables\n",
    "\n",
    "**confusion matrix** help us where we are confused\n",
    "\n",
    "**ImageClassifierCleaner** let us find for example two bears top confused things\n",
    "\n",
    "Model helping you and then go ahead train data after cleaning\n",
    "\n",
    "\n",
    "Notebook4 included loss function\n",
    "Fastai atomatically pick good loss function\n",
    "\n",
    "Lets look what acc. it picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "14309541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlattenedLoss of CrossEntropyLoss()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70845726",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Same as Mnist loss ..kind of extended version\n",
    "\n",
    "torch.where only works when you have binary outcome\n",
    "\n",
    "we want to create just like that but we want to make it work more than two categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006ed3f",
   "metadata": {},
   "source": [
    "## Lets see whats inside Batch\n",
    "\n",
    "destructure\n",
    "batch size = 64\n",
    "\n",
    "dls.vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32d2df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dls.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "acbe9cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorCategory([ 0, 25, 33, 19, 12, 22, 35, 17, 23, 32,  8,  5,  3, 33, 36, 34, 32,  7,  1, 14,  5, 14,  8, 36, 18, 13, 22,  5,  0, 20, 18, 33, 28, 28, 19, 33, 26,  0, 30, 25, 27, 23, 31,  1, 17, 13,  8, 23,\n",
       "        34, 24, 28,  7, 13, 12, 31, 10, 29, 33, 22,  0, 21, 20,  3,  6], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1af209f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British_Shorthair', 'Egyptian_Mau', 'Maine_Coon', 'Persian', 'Ragdoll', 'Russian_Blue', 'Siamese', 'Sphynx', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'boxer', 'chihuahua', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond', 'leonberger', 'miniature_pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63ec8025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boxer'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.vocab[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6dd60",
   "metadata": {},
   "source": [
    "## View the predictions\n",
    "its just call the last activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a7f6fdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([9.5199e-01, 1.0111e-02, 6.5786e-05, 2.1975e-04, 7.8775e-04, 3.6360e-03, 3.0038e-03, 1.0312e-04, 2.4044e-02, 5.6874e-04, 5.1095e-05, 3.3239e-03, 2.5059e-05, 5.7563e-06, 1.0179e-05, 9.1572e-06,\n",
       "        8.6417e-06, 2.3385e-04, 1.0258e-05, 1.3129e-05, 7.4432e-06, 8.3646e-06, 5.2068e-05, 1.6303e-05, 4.9455e-06, 6.5756e-06, 6.5956e-05, 9.1565e-06, 3.6239e-05, 1.1460e-05, 1.4704e-05, 3.3519e-05,\n",
       "        9.6280e-04, 3.4956e-04, 7.9019e-07, 1.6408e-05, 1.7885e-04])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds,_ = learn.get_preds(dl= [(x,y)])\n",
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c8a0948c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, tensor(1.0000))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds[0]), preds[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b56cc",
   "metadata": {},
   "source": [
    "## How  do we go about this prediction\n",
    "\n",
    "Softmax is an extension of sigmoid handle more than two categoreis\n",
    "\n",
    "what if we want 37 cat.\n",
    "\n",
    "we need one activation for 1 category\n",
    " e.g in case 3,7 activations are two\n",
    " \n",
    " \n",
    " below 1st column is activation of 1st cat and 2nd is for 7\n",
    " \n",
    " like how much like 3 and how much like 7\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ddb92622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6734,  0.2576],\n",
       "        [ 0.4689,  0.4607],\n",
       "        [-2.2457, -0.3727],\n",
       "        [ 4.4164, -1.2760],\n",
       "        [ 0.9233,  0.5347],\n",
       "        [ 1.0698,  1.6187]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(42),\n",
    "acts = torch.randn((6,2))*2\n",
    "acts\n",
    "\n",
    "#How much likely is first and how muc to 7 i.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14c5c7",
   "metadata": {},
   "source": [
    "## Taking Sigmoid\n",
    "\n",
    "if we take it values will be between 0 or 1\n",
    "\n",
    "\n",
    "but dont add up to one\n",
    "\n",
    "so doesnt make sense\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d56f6f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6623, 0.5641],\n",
       "        [0.6151, 0.6132],\n",
       "        [0.0957, 0.4079],\n",
       "        [0.9881, 0.2182],\n",
       "        [0.7157, 0.6306],\n",
       "        [0.7446, 0.8346]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e120f",
   "metadata": {},
   "source": [
    "## Solution :\n",
    "\n",
    "\n",
    "1. So if we take difference \n",
    "\n",
    "2. Relative confidence: take sigmoid after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "41b1e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = acts[:,0]- acts[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ecd48e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4158,  0.0083, -1.8731,  5.6924,  0.3886, -0.5489])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b925ff03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6025, 0.3975],\n",
       "        [0.5021, 0.4979],\n",
       "        [0.1332, 0.8668],\n",
       "        [0.9966, 0.0034],\n",
       "        [0.5959, 0.4041],\n",
       "        [0.3661, 0.6339]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([diff.sigmoid(),1-diff.sigmoid()],dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389065d9",
   "metadata": {},
   "source": [
    "## More than 2 cat:\n",
    "\n",
    "Use Softmax\n",
    "\n",
    "in binary case it is equal to sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569e3fa",
   "metadata": {},
   "source": [
    "The second column (the probability of it being a 7) will then just be that value subtracted from 1. Now, we need a way to do all this that also works for more than two columns. It turns out that this function, called `softmax`, is exactly that:\n",
    "\n",
    "``` python\n",
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df428b",
   "metadata": {},
   "source": [
    "> jargon: Exponential function (exp): Literally defined as `e**x`, where `e` is a special number approximately equal to 2.718. It is the inverse of the natural logarithm function. Note that `exp` is always positive, and it increases _very_ rapidly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb090ce",
   "metadata": {},
   "source": [
    "Let's check that `softmax` returns the same values as `sigmoid` for the first column, and those values subtracted from 1 for the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "86a716a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6025, 0.3975],\n",
       "        [0.5021, 0.4979],\n",
       "        [0.1332, 0.8668],\n",
       "        [0.9966, 0.0034],\n",
       "        [0.5959, 0.4041],\n",
       "        [0.3661, 0.6339]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_acts = torch.softmax(acts, dim=1)\n",
    "sm_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96cc22",
   "metadata": {},
   "source": [
    "`softmax` is the multi-category equivalent of `sigmoid`—we have to use it any time we have more than two categories and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent. We could create other functions that have the properties that all activations are between 0 and 1, and sum to 1; however, no other function has the same relationship to the sigmoid function, which we've seen is smooth and symmetric. Also, we'll see shortly that the softmax function works well hand-in-hand with the loss function we will look at in the next section.\n",
    "\n",
    "If we have three output activations, such as in our bear classifier, calculating softmax for a single bear image would then look like something like <<bear_softmax>>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3ae100bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<img alt=\"Bear softmax example\" width=\"280\" id=\"bear_softmax\" caption=\"Example of softmax on the bear classifier\" src=\"images/att_00062.png\">\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "<img alt=\"Bear softmax example\" width=\"280\" id=\"bear_softmax\" caption=\"Example of softmax on the bear classifier\" src=\"images/att_00062.png\">\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd220f3e",
   "metadata": {},
   "source": [
    "## Interesting thing\n",
    "\n",
    "e to power something grows really fast\n",
    "see below\n",
    "\n",
    "if we have one activation is bit bigger than other\n",
    "\n",
    "then softmax is really big\n",
    "\n",
    "its tries to pick one whcih one \n",
    "\n",
    "\n",
    "thats not you always want sometimes you have inference time you want to bit concious\n",
    "\n",
    "its default you do most of time\n",
    "\n",
    "so that is somtmax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7c023f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.598150033144236"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505b4b",
   "metadata": {},
   "source": [
    "math.exp(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f6a42697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.4287934927351"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5eb2a3",
   "metadata": {},
   "source": [
    "## Log Likelihood\n",
    "\n",
    "binary case. we did this\n",
    "\n",
    "```python\n",
    "def mnist_loss(inputs, targets):\n",
    "    inputs = inputs.sigmoid()\n",
    "    return torch.where(targets==1, 1-inputs, inputs).mean()\n",
    "```\n",
    "\n",
    "its fine it worked so we could do thing exactly same thing\n",
    "\n",
    "because tagets are not 0 or 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2ac05307",
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = tensor([0,1,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6475f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6025, 0.3975],\n",
       "        [0.5021, 0.4979],\n",
       "        [0.1332, 0.8668],\n",
       "        [0.9966, 0.0034],\n",
       "        [0.5959, 0.4041],\n",
       "        [0.3661, 0.6339]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_acts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86465e",
   "metadata": {},
   "source": [
    "## Replace torch.where\n",
    "\n",
    "1. grab all number from 0-5\n",
    "2. my targets \n",
    "3. each row number it will pick particular column defined in target\n",
    "4. lets see pick column 0  for first row\n",
    "\n",
    "\n",
    "so this is super nifty indexing expression you should play with\n",
    "\n",
    "1st thing say which row you should return\n",
    "\n",
    "second says which column\n",
    "\n",
    "we can use that more than two values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61206034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = range(6)\n",
    "sm_acts[idx,targ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a3127df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table >\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"col_heading level0 col0\" >3</th>\n",
       "      <th class=\"col_heading level0 col1\" >7</th>\n",
       "      <th class=\"col_heading level0 col2\" >target</th>\n",
       "      <th class=\"col_heading level0 col3\" >idx</th>\n",
       "      <th class=\"col_heading level0 col4\" >loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_353e2_row0_col0\" class=\"data row0 col0\" >0.602469</td>\n",
       "      <td id=\"T_353e2_row0_col1\" class=\"data row0 col1\" >0.397531</td>\n",
       "      <td id=\"T_353e2_row0_col2\" class=\"data row0 col2\" >tensor(0)</td>\n",
       "      <td id=\"T_353e2_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_353e2_row0_col4\" class=\"data row0 col4\" >tensor(0.6025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_353e2_row1_col0\" class=\"data row1 col0\" >0.502065</td>\n",
       "      <td id=\"T_353e2_row1_col1\" class=\"data row1 col1\" >0.497935</td>\n",
       "      <td id=\"T_353e2_row1_col2\" class=\"data row1 col2\" >tensor(1)</td>\n",
       "      <td id=\"T_353e2_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "      <td id=\"T_353e2_row1_col4\" class=\"data row1 col4\" >tensor(0.4979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_353e2_row2_col0\" class=\"data row2 col0\" >0.133188</td>\n",
       "      <td id=\"T_353e2_row2_col1\" class=\"data row2 col1\" >0.866811</td>\n",
       "      <td id=\"T_353e2_row2_col2\" class=\"data row2 col2\" >tensor(0)</td>\n",
       "      <td id=\"T_353e2_row2_col3\" class=\"data row2 col3\" >2</td>\n",
       "      <td id=\"T_353e2_row2_col4\" class=\"data row2 col4\" >tensor(0.1332)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_353e2_row3_col0\" class=\"data row3 col0\" >0.996640</td>\n",
       "      <td id=\"T_353e2_row3_col1\" class=\"data row3 col1\" >0.003360</td>\n",
       "      <td id=\"T_353e2_row3_col2\" class=\"data row3 col2\" >tensor(1)</td>\n",
       "      <td id=\"T_353e2_row3_col3\" class=\"data row3 col3\" >3</td>\n",
       "      <td id=\"T_353e2_row3_col4\" class=\"data row3 col4\" >tensor(0.0034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_353e2_row4_col0\" class=\"data row4 col0\" >0.595949</td>\n",
       "      <td id=\"T_353e2_row4_col1\" class=\"data row4 col1\" >0.404051</td>\n",
       "      <td id=\"T_353e2_row4_col2\" class=\"data row4 col2\" >tensor(1)</td>\n",
       "      <td id=\"T_353e2_row4_col3\" class=\"data row4 col3\" >4</td>\n",
       "      <td id=\"T_353e2_row4_col4\" class=\"data row4 col4\" >tensor(0.4041)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_353e2_row5_col0\" class=\"data row5 col0\" >0.366118</td>\n",
       "      <td id=\"T_353e2_row5_col1\" class=\"data row5 col1\" >0.633882</td>\n",
       "      <td id=\"T_353e2_row5_col2\" class=\"data row5 col2\" >tensor(0)</td>\n",
       "      <td id=\"T_353e2_row5_col3\" class=\"data row5 col3\" >5</td>\n",
       "      <td id=\"T_353e2_row5_col4\" class=\"data row5 col4\" >tensor(0.3661)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "df = pd.DataFrame(sm_acts , columns=[\"3\",\"7\"])\n",
    "df[\"target\"] = targ\n",
    "df[\"idx\"] = idx\n",
    "df[\"loss\"] = sm_acts[range(6), targ]\n",
    "t= df.style.hide_index()\n",
    "\n",
    "#To have html code compatible with our script\n",
    "html = t._repr_html_().split('</style>')[1]\n",
    "html = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\n",
    "display(HTML(html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae11422",
   "metadata": {},
   "source": [
    "## How to make it Work More than 2 colums\n",
    "\n",
    "1. full mnist : we will have more 10 columns\n",
    "2. indexer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db2bea",
   "metadata": {},
   "source": [
    "## Negative Log Liklihood\n",
    "\n",
    "There is no log in it we will see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b33f65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-sm_acts[idx, targ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e204a3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(sm_acts,targ,reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c458b2",
   "metadata": {},
   "source": [
    "## Lets Talk about Logs\n",
    "\n",
    "\n",
    "**Problem : That means that our model will not care whether it predicts 0.99 or 0.999. Indeed, those numbers are so close together but in another sense, 0.999 is 10 times more confident than 0.99**\n",
    "\n",
    "\n",
    "\n",
    "The function we saw in the previous section works quite well as a loss function, but we can make it a bit better. The problem is that we are using probabilities, and probabilities cannot be smaller than 0 or greater than 1. That means that our model will not care whether it predicts 0.99 or 0.999. Indeed, those numbers are so close together—but in another sense, 0.999 is 10 times more confident than 0.99. So, we want to transform our numbers between 0 and 1 to instead be between negative infinity and infinnity. There is a mathematical function that does exactly this: the *logarithm* (available as `torch.log`). It is not defined for numbers less than 0, and looks like this:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "so this log fuctoin\n",
    "\n",
    "we can acc make it better\n",
    "\n",
    "what if model decide 0.99 or 0.999\n",
    "\n",
    "if we have 1000 things then right one is better than 0.99\n",
    "\n",
    "\n",
    "so really we like to transform numbers between 0-1 instead -infinite to positive infinite\n",
    "\n",
    "log will help us in this case\n",
    "\n",
    "\n",
    "so \n",
    "\n",
    "numbers as we closer to zero its goes down to infinity at 1 it goes to zeros\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7d8c6405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-4.6052), tensor(0.), tensor(-inf))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(tensor(0.01)), torch.log(tensor(1)),torch.log(tensor(0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b06924",
   "metadata": {},
   "source": [
    "we cant go zero\n",
    "\n",
    "our loss function we want to be negative\n",
    "\n",
    "\n",
    "(y = b power a)\n",
    "a= log(y,b)\n",
    "\n",
    "\n",
    "what intersting\n",
    "\n",
    "log (a*b) = log(a) + log(b)\n",
    "\n",
    "a*b can be very very big or small\n",
    "\n",
    "adding not get out of control\n",
    "\n",
    "when we take the probabilities such as sm_acts\n",
    "\n",
    "1. we take log \n",
    "\n",
    "2. we take mean\n",
    "\n",
    "that is called negative log likelihood\n",
    "\n",
    "\n",
    "**if we take softmax \n",
    "and then log*\n",
    "\n",
    "pass to nll_loss\n",
    "\n",
    "\n",
    "which is cross Entropy Loss\n",
    "\n",
    "\n",
    "why nll_loss dont take Log?\n",
    "\n",
    "\n",
    "the reason it is more convinient to take log back at softmax step\n",
    "\n",
    "\n",
    "so pytorch has fuction. log_softmax since it is very easir pytirch assume u did log softmax and pass to nll loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1d13de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_function(torch.log, min=0,max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba527783",
   "metadata": {},
   "source": [
    "## Two ways for Cross entropy Loss\n",
    "\n",
    "\n",
    "single number because of mean\n",
    "\n",
    "reduction = none \n",
    "\n",
    "for looking all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d0557132",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "821369a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8045)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(acts,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f8af58e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8045)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(acts,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c758a947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss(reduction=\"none\")(acts,targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa0ad4",
   "metadata": {},
   "source": [
    "## why loss function needs to negative?\n",
    "\n",
    "Lower it is better \n",
    "\n",
    "needed to cuttoff\n",
    "\n",
    "\n",
    "next week Data Ethics:\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
